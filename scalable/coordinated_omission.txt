****************************
*** Coordinated Omission ***
****************************

CO - Coordinated Omission
SUT - System Under Test
LT - Load Tester

Goal:
	LDBC SNB Interactive is designed to benchmark databases under interactive/online workloads, where latency under a given load is arguably more interesting than knowing throughput

Papers:
	http://users.cms.caltech.edu/~adamw/papers/openvsclosed.pdf

People:
	Gil Tene, CTO & co-founder of Azul Systems

Summary:
	Coordinated Omission (CO) is a measurement methodology problem that applies to performance measuring systems, e.g. load testers (LT), rather than to the systems under test (SUT). It is a common problem, leading to the incorrect reporting of (especially higher) percentile data.

	The problem occurs when an observed response time is longer than the interval that the LT is trying to maintain between requests; when an LT that waits for a response before issuing the next request misses the time window in which it would have sent that next request, measurements are omitted. Whenever the blocking of a thread in an LT is somehow coordinated with an SUT event, CO is likely to occur. The term 'coordinated' is used in the sense that the LT pauses at the same time the SUT experiences heavy load and, with high probability, the SUT would take longer than usual to respond to other requests if the LT was able to (was not paused) send them at that time. 

	This equates to the LT allowing the SUT to say "I'm heavily loaded right now, please let me catch up before sending more requests". 
	In effect, the LT omits slow responses that would likely occur if it sent the requests as intended, resulting in incorrect higher percentile measurements because the calculations do not include the slow responses that were 'omitted'. It is analogous to removing a large percentage of the slow responses from the results (typically only the first slow response in each sequence of slow responses survives the removal), leaving almost only the faster responses. 
	This skews all statistics (e.g. percentiles) calculated from the recorded results.
	Statistics can be orders of magnitude off or, for example, the reported 99.9th percentile may appear better than the real 99th percentile.

	Consider the following scenario.
	A test is constructed to measure the performance of an SUT under real world conditions, and the metric deemed most important is 90th percentile latency.
	In real world conditions the SUT would receive 1 request every 1s, so the LT is configured to do the same.
	The LT works as follows: send request to SUT, wait for response from SUT, measure response time, repeat until end of workload then report response times as percentiles.
	Note that while waiting for a response the LT does nothing, i.e., it sends no new requests during this time.
	Normally the SUT responds in 900ms or less, but every 900s it takes as long as 100s due to the overhead of some periodic system maintenance (e.g. garbage collection).
	Given this scenario, every 1000s period the LT would receive 900 x 900ms responses and 1 x 100s response, and it would report a 90th percentile of 900ms.
	This report is erroneous.
	Consider the following: 
	during the 100s pause the SUT performance appears to be markedly (~100x) worse,
	during the first 900s of every 1000s period 900 responses are recorded, but during the last 100s only 1 is recorded, 10% of the time (when SUT performance is at its worst) is represented by only ~0.1% (1/901) of the recorded results.
	Had the LT continued sending requests every 1s during the 100s slow period, results for each 1000s period would likely contain 900 x 900ms responses and 100 x 100s responses, translating to a 90th percentile of 100s, 2 orders of magnitude higher that what was actually reported.

	Observe, these results are clearly invalid, but it is only because the omitted requests were due to pauses in the SUT that the cause of error is CO.
	Had the LT omitted requests at random the results may have remained valid, they may still have been representative of the SUT's real world behavior: an uncoordinated sample of the SUT response times.

	LT pause omissions

	Further, random skips/omissions are fine, as long as they remain uncoordinated. Pauses in the LT that are not coordinated with the SUT do not result in anything being missed, from a statistical perspective. It is only when the LT skips/omits requests as a result of long response times from the SUT that coordination is introduced into the test, which is when the results become skewed.
	UNCOORDINATED SKIPS STILL RESULT IN THE SUT EXPERIENCING A LESSSER LOAD


**************************************************

Maybe Summary:
	- As long as LT has not skipped/omitted requests from the expected workload the test data remains valid: an uncoordinated sample of the SUT response times, and it is just necessary to have enough data such that the reported results are statistically significant. 
	Further, random skips/omissions are fine, as long as they remain uncoordinated. Pauses in the LT that are not coordinated with the SUT do not result in anything being missed, from a statistical perspective. It is only when the LT skips/omits requests as a result of long response times from the SUT that coordination is introduced into the test, which is when the results become skewed.
	UNCOORDINATED SKIPS STILL RESULT IN THE SUT EXPERIENCING A LESSSER LOAD

	- If during benchmark execution something (e.g. garbage collection) causes the SUT to momentarily pause, and the LT wait (block) for the SUT to resume before sending more requests (they are coordinated), only N requests would show the full pause time (e.g. 1 second), all other requests would show un-paused time (e.g. 1 ms). However in a real application it is unlikely that clients will recognize the SUT is under load and stop sending requests until it recovers. It is more likely they will continue sending requests during that time. In that case all clients will be affected that pause. The issue of CO is how many requests are truly impacted by the pause versus the N that appear in benchmark results (due to omitted results).

Wordy:
	- "is it ok for me to test your latency right now? Because if it's not, I'm fine waiting and testing it only when you are ready..."

Motivation/value:
	- As reality does not omit bad results in this way, coordinated omission results in poor decision-making during capacity planning, setting service level expectations, software selection, etc.

Correction:
	- Correcting CO: 
	  -  Avoid: 
	  	- The best way to correct the problem is to avoid it from occurring (e.g. by designing an LT that sends requests at an explicitly defined interval, that never responds to any form of back pressure)
	  	- As such, higher thread count (assuming sufficient compute resources) generally relieves the problem as it results in less likelihood of one thread having to wait for another to complete, and coordinated omission will have less impact.
	  	- Non-blocking driver design
	  -  Correct: 
	  	- LT that have a known interval between requests are easiest to correct. When the expected test scenario is known it is possible to correct for it when the LT does not act as expected, e.g. use expected interval between requests to re-create the missing/omitted requests when omitted requests are detected. This simply generates "more correct" data (filling in the gaps), resulting in "more correct" percentile calculation results.
	  	- Challenge: when correcting with injected/fake requests, it is difficult knowing what response time to report for those requests: mean/X-percentile until now, and should/shouldn't previous injected/fake requests used for this calculation
	  - Limitation: 
	  	- A problem with "Correct" approach is it approximates what SUT behavior would be like had the omitted requests actually occurred, but in actuality the system may have failed entirely due to the additional load.
	  	- Estimating the omitted requests (based on response time, average latencies, etc.) and injecting them into the results somewhat corrects the reported measurements, however does not account for the effect those missing requests would have imposed on the system: queueing effects, cache pressures, potential buffer exhaustion, etc. Those requests would have placed additional load on system more, perhaps even causing it to fail.

	- Coordinated omissions causes incorrect measurements because the reported percentiles do not account for the omitted results. Estimating the omitted requests (based on response time, average latencies, etc.) and injecting them into the results somewhat corrects the reported measurements, however does not account for the effect those missing requests would have imposed on the system: queueing effects, cache pressures, potential buffer exhaustion, etc. Those requests would have placed additional load on system more, perhaps even causing it to fail.

	- Once an LT misses an opportunity to send a request on time there is no way of knowing what would have happened if it had. Though some correction techniques are better than others, they are all estimates from that point.

Driver design & latency vs throughput:
	- A LT will be subject to the CO problem whether it sends requests at a specific interval or back-to-back as-fast-as-possible. However, an as-fast-as-possible LT is more likely to suffer from CO, as its "typical" interval is very short, therefore it will miss more sampling opportunities than a system with some amount of "think time".
	An as-fast-as-possible LT still has an expected interval between requests, to some extent. However, this interval will differ depending on the SUT, and it will need to be computed (e.g. a good estimate is the ongoing average response time, or preferably estimate the ongoing average with some type of outlier filtering)

Coordinated vs Uncoordinated omissions:
	- Coordinated: as discussed above, i.e. CO

	- Uncoordinated: still gets a random sample of the SUT performance, meaning CO issues are avoided, but the benchmark becomes non-deterministic (dependent on the hardware that LT runs on) and likely non-repeatable, making it difficult to compare multiple SUT using the same workload

	- As long as LT has not skipped/omitted requests from the expected workload the test data remains valid: an uncoordinated sample of the SUT response times, and it is just necessary to have enough data such that the reported results are statistically significant. Further, random skips/omissions are fine, as long as they remain uncoordinated. Pauses in the LT that are not coordinated with the SUT do not result in anything being missed, from a statistical perspective. It is only when the LT skips/omits requests as a result of long response times from the SUT that coordination is introduced into the test, which is when the results become skewed.

* Possibly cite Twitter Iago tool as good

* Possibly cite SPEC(?), TPC(?), LDBC(?), others(?), as bad

* TCP back pressure - we avoid it by having handlers and including that cost in the measurement.... blah blah... or maybe don't even metion that

*******************
*** ldbc_driver ***
*******************

- goal is to be deterministic: if driver, even when not caused by system under test, can not keep up with configured workload it will fail
- goal is to test the configured workload: if driver, possibly but not necessarily caused by the system under test, is prevented from generating events according to the configured workload it will fail

*******************************
*** ldbc_driver future work ***
*******************************

LT could correct for omitted results due to coordination with SUT, in order to avoid the effects of CO in the results. LT must know what it should have done, and use that knowledge to decide what results to record. This is possible when LT picks random or even varying interval scenarios, as the LT always knows what should have occurred according to the configured workload. The LT acts according to a configured workload, tracks where it is in the workload, and computes injected/fake results based on that plan. 

Estimate Correction
	When a delay to execute an operation on time occurs:
	 - calculate how long it was delayed
	 - calculate the operations that would have been executed in that time
	 -- based on the workload mix
	 -- based on interval between operations
	 -- could do this by creating a (small) sample of the full workload during driver initialization, creating a looping iterator over that sample, then injecting/logging-to-metrics "fake" operations from this iterator, to account for the "omitted" operations
	 --- may need multiple samples from different workload timeframes to account for workloads that change over time
	 --- based on which operation failed, choose appropriate sample from appropriate timeframe
	 --- need to estimate scheduledStartTime, actualStartTime (perhaps equal to scheduledStartTime or perhaps equal to mean from previous operations?), runDuration (taken from mean of previous operations, for example)
	 --- would means from previous operations include the times from the injected/fake operations?

	Alternatively, when a delay to execute an operation on time occurs:
	 - rather than executing that operation, send it to a "missed operations" place
	 - do not execute it
	 - do not log the operation in any other way
	 - tolerate up to some amount of such operations
	 - writes would still have to be executed, but reads do not need to be
	 - upon completion of the benchmark assign a some start and finish time to all "missed operations", based on means/percentiles/whatever of other operations of the same type/that were executed at the same period/whatever. place these in "corrected operations"
	 - include metrics from "corrected operations" to the final report metrics

	Could possibly do this from a Delay Policy implementation

	If the metrics logging service has this looping iterator the "fake" operations would not need to all get sent over a queue to the logging thread. Instead, just 1 message, containing a parameter communicating "number of fake operations to create", would be sent to the logging thread and it would deal with the rest

	Make configurable: 
	 - terminate
	 - correct from sample
	 - something else?
	 - ...

Report execution duration based on when SCHEDULED to send and when actually received?
Problems:
	- SUT not understander same stress as it would have been had driver sent that request